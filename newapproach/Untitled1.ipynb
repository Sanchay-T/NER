{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__fLxUPhG1X6",
        "outputId": "4b75544b-429b-4b2e-d6bf-dd37ca7f8fe3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy tqdm\n",
        "!python -m spacy download en_core_web_sm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import json\n",
        "from typing import List, Dict, Tuple, Any\n",
        "from spacy.training import Example\n",
        "from spacy.lang.en import English\n",
        "\n",
        "class NERTrainer:\n",
        "    def __init__(self):\n",
        "        print(\"\\n=== Initializing NER Trainer ===\")\n",
        "\n",
        "    def load_data(self, filepath: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Load and validate data from JSON file.\"\"\"\n",
        "        print(f\"Loading and validating training data from: {filepath}\")\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "        # Filter valid examples and augment data if necessary\n",
        "        valid_data = [item for item in data if self.validate_example(item)]\n",
        "        print(f\"Loaded {len(data)} examples, {len(valid_data)} valid after cleaning\")\n",
        "        augmented_data = self.augment_data(valid_data)\n",
        "        print(f\"Augmented data size: {len(augmented_data)} examples\")\n",
        "        return augmented_data\n",
        "\n",
        "    def validate_example(self, example: Dict[str, Any]) -> bool:\n",
        "        \"\"\"Placeholder validation for a single training example.\"\"\"\n",
        "        return 'text' in example and 'entities' in example\n",
        "\n",
        "    def augment_data(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Placeholder for data augmentation.\"\"\"\n",
        "        # Placeholder implementation; in real scenarios, could add noise or synonyms\n",
        "        return data\n",
        "\n",
        "    def prepare_training_data(self, data: List[Dict], split: float = 0.2) -> Tuple[List[Dict], List[Dict]]:\n",
        "        \"\"\"Split data into training and validation sets\"\"\"\n",
        "        print(\"\\nPreparing training and validation splits...\")\n",
        "        random.shuffle(data)\n",
        "        split_point = int(len(data) * (1 - split))\n",
        "        train_data = data[:split_point]\n",
        "        val_data = data[split_point:]\n",
        "        print(f\"Training examples: {len(train_data)}\")\n",
        "        print(f\"Validation examples: {len(val_data)}\")\n",
        "        return train_data, val_data\n",
        "\n",
        "    def train_model(self, train_data: List[Dict], n_iter: int = 10):\n",
        "        \"\"\"Train NER model\"\"\"\n",
        "        nlp = English()  # Load blank model or existing one\n",
        "        # Add necessary pipeline components and training loop\n",
        "        print(\"\\nTraining model...\")\n",
        "\n",
        "        # Training loop logic here\n",
        "        # Placeholder for demonstration\n",
        "        for i in range(n_iter):\n",
        "            print(f\"Iteration {i+1}/{n_iter}\")\n",
        "            # Simulate training process\n",
        "\n",
        "    def evaluate_model(self, nlp, examples: List[Example]) -> Dict:\n",
        "        \"\"\"Evaluate model performance\"\"\"\n",
        "        print(\"\\nEvaluating model...\")\n",
        "        scores = {\n",
        "            \"ents_p\": 0,\n",
        "            \"ents_r\": 0,\n",
        "            \"ents_f\": 0,\n",
        "            \"ents_per_type\": {}\n",
        "        }\n",
        "\n",
        "        for example in examples:\n",
        "            pred_ents = nlp(example.reference.text).ents\n",
        "            gold_ents = example.reference.ents\n",
        "\n",
        "            # Calculate true positives, false positives, and false negatives\n",
        "            tp = len([ent for ent in pred_ents if any(\n",
        "                gold_ent.label_ == ent.label_ and\n",
        "                gold_ent.start == ent.start and\n",
        "                gold_ent.end == ent.end\n",
        "                for gold_ent in gold_ents\n",
        "            )])\n",
        "            fp = len(pred_ents) - tp\n",
        "            fn = len(gold_ents) - tp\n",
        "\n",
        "            # Calculate precision, recall, and F1\n",
        "            p = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "            r = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "            f = 2 * (p * r) / (p + r) if (p + r) > 0 else 0\n",
        "\n",
        "            scores[\"ents_p\"] += p\n",
        "            scores[\"ents_r\"] += r\n",
        "            scores[\"ents_f\"] += f\n",
        "\n",
        "        # Calculate averages\n",
        "        n = len(examples) if len(examples) > 0 else 1\n",
        "        scores[\"ents_p\"] /= n\n",
        "        scores[\"ents_r\"] /= n\n",
        "        scores[\"ents_f\"] /= n\n",
        "\n",
        "        print(\"\\nEvaluation results:\")\n",
        "        print(f\"Precision: {scores['ents_p']}\")\n",
        "        print(f\"Recall: {scores['ents_r']}\")\n",
        "        print(f\"F1 Score: {scores['ents_f']}\")\n",
        "\n",
        "        return scores\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(\"\\n=== Initializing NER Trainer ===\")\n",
        "    trainer = NERTrainer()\n",
        "\n",
        "    # Load data\n",
        "    data = trainer.load_data(\"spacy_training_data.json\")\n",
        "\n",
        "    # Split data\n",
        "    train_data, val_data = trainer.prepare_training_data(data)\n",
        "\n",
        "    # Train model\n",
        "    trainer.train_model(train_data, n_iter=10)\n",
        "\n",
        "    # Evaluate model with dummy data for demonstration\n",
        "    # You would convert val_data into spaCy Example objects for real evaluation\n",
        "    nlp = English()  # Replace with your trained model\n",
        "    dummy_examples = []  # Placeholder for spaCy Example objects for validation data\n",
        "    scores = trainer.evaluate_model(nlp, dummy_examples)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFiUCTu-G_Cf",
        "outputId": "26875ee5-a0ed-4c47-b203-95a0cb4b59b4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Initializing NER Trainer ===\n",
            "\n",
            "=== Initializing NER Trainer ===\n",
            "Loading and validating training data from: spacy_training_data.json\n",
            "Loaded 17 examples, 17 valid after cleaning\n",
            "Augmented data size: 17 examples\n",
            "\n",
            "Preparing training and validation splits...\n",
            "Training examples: 13\n",
            "Validation examples: 4\n",
            "\n",
            "Training model...\n",
            "Iteration 1/10\n",
            "Iteration 2/10\n",
            "Iteration 3/10\n",
            "Iteration 4/10\n",
            "Iteration 5/10\n",
            "Iteration 6/10\n",
            "Iteration 7/10\n",
            "Iteration 8/10\n",
            "Iteration 9/10\n",
            "Iteration 10/10\n",
            "\n",
            "Evaluating model...\n",
            "\n",
            "Evaluation results:\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F1 Score: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "from spacy.training import Example\n",
        "from spacy.util import minibatch, compounding\n",
        "from pathlib import Path\n",
        "import json\n",
        "import random\n",
        "from typing import List, Dict, Tuple\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "class NERTrainer:\n",
        "    def __init__(self):\n",
        "        print(\"\\n=== Initializing NER Trainer ===\")\n",
        "\n",
        "    def clean_text_and_entities(self, text: str, entities: List[Tuple[int, int, str]]) -> Tuple[str, List[Tuple[int, int, str]]]:\n",
        "        \"\"\"Clean text and adjust entity positions\"\"\"\n",
        "        # Normalize whitespace\n",
        "        lines = [line.strip() for line in text.split('\\n')]\n",
        "        clean_text = '\\n'.join(line for line in lines if line)\n",
        "\n",
        "        # Adjust entity positions\n",
        "        adjusted_entities = []\n",
        "        for start, end, label in entities:\n",
        "            entity_text = text[start:end].strip()\n",
        "            new_start = clean_text.find(entity_text)\n",
        "            if new_start != -1:\n",
        "                adjusted_entities.append((\n",
        "                    new_start,\n",
        "                    new_start + len(entity_text),\n",
        "                    label\n",
        "                ))\n",
        "\n",
        "        return clean_text, adjusted_entities\n",
        "\n",
        "    def validate_entity(self, text: str, start: int, end: int, label: str) -> bool:\n",
        "        \"\"\"Validate if entity position is correct\"\"\"\n",
        "        if start >= 0 and end <= len(text) and start < end:\n",
        "            entity_text = text[start:end].strip()\n",
        "            if label == \"ACC_NO\":\n",
        "                # Account number should contain digits\n",
        "                return bool(re.search(r'\\d', entity_text))\n",
        "            elif label == \"PER\":\n",
        "                # Name should contain letters\n",
        "                return bool(re.search(r'[A-Za-z]', entity_text))\n",
        "        return False\n",
        "\n",
        "    def load_and_clean_data(self, file_path: str) -> List[Dict]:\n",
        "        \"\"\"Load and clean training data\"\"\"\n",
        "        print(f\"\\nLoading and cleaning data from: {file_path}\")\n",
        "\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        cleaned_data = []\n",
        "        for item in data:\n",
        "            # Clean text and adjust entities\n",
        "            clean_text, adjusted_entities = self.clean_text_and_entities(\n",
        "                item[\"text\"],\n",
        "                item[\"entities\"]\n",
        "            )\n",
        "\n",
        "            # Validate entities\n",
        "            valid_entities = [\n",
        "                (start, end, label)\n",
        "                for start, end, label in adjusted_entities\n",
        "                if self.validate_entity(clean_text, start, end, label)\n",
        "            ]\n",
        "\n",
        "            if valid_entities:\n",
        "                cleaned_data.append({\n",
        "                    \"text\": clean_text,\n",
        "                    \"entities\": valid_entities\n",
        "                })\n",
        "\n",
        "        print(f\"Loaded {len(data)} examples, cleaned to {len(cleaned_data)} valid examples\")\n",
        "        return cleaned_data\n",
        "\n",
        "    def augment_data(self, data: List[Dict]) -> List[Dict]:\n",
        "        \"\"\"Augment training data with variations\"\"\"\n",
        "        print(\"\\nAugmenting training data...\")\n",
        "        augmented_data = []\n",
        "\n",
        "        for item in data:\n",
        "            text = item[\"text\"]\n",
        "            entities = item[\"entities\"]\n",
        "\n",
        "            # Add original example\n",
        "            augmented_data.append(item)\n",
        "\n",
        "            # Add variations only for account numbers\n",
        "            acc_variations = []\n",
        "            for start, end, label in entities:\n",
        "                if label == \"ACC_NO\":\n",
        "                    entity_text = text[start:end]\n",
        "\n",
        "                    # Remove all spaces\n",
        "                    no_spaces = entity_text.replace(\" \", \"\")\n",
        "                    if no_spaces != entity_text:\n",
        "                        new_text = text[:start] + no_spaces + text[end:]\n",
        "                        acc_variations.append({\n",
        "                            \"text\": new_text,\n",
        "                            \"entities\": [(start, start + len(no_spaces), \"ACC_NO\")]\n",
        "                        })\n",
        "\n",
        "                    # Add spaces every 4 digits\n",
        "                    spaced = ' '.join(re.findall('.{1,4}', no_spaces))\n",
        "                    if spaced != entity_text:\n",
        "                        new_text = text[:start] + spaced + text[end:]\n",
        "                        acc_variations.append({\n",
        "                            \"text\": new_text,\n",
        "                            \"entities\": [(start, start + len(spaced), \"ACC_NO\")]\n",
        "                        })\n",
        "\n",
        "            # Add valid variations\n",
        "            for var in acc_variations:\n",
        "                if all(self.validate_entity(var[\"text\"], start, end, label)\n",
        "                      for start, end, label in var[\"entities\"]):\n",
        "                    augmented_data.append(var)\n",
        "\n",
        "        print(f\"Original examples: {len(data)}\")\n",
        "        print(f\"Augmented examples: {len(augmented_data)}\")\n",
        "        return augmented_data\n",
        "\n",
        "    def train_model(self, train_data: List[Dict], val_data: List[Dict],\n",
        "                   output_dir: str, n_iter: int = 100):\n",
        "        \"\"\"Train the NER model\"\"\"\n",
        "        print(\"\\nStarting model training...\")\n",
        "\n",
        "        # Initialize blank English model\n",
        "        nlp = spacy.blank(\"en\")\n",
        "\n",
        "        # Add NER pipeline\n",
        "        if \"ner\" not in nlp.pipe_names:\n",
        "            ner = nlp.add_pipe(\"ner\")\n",
        "\n",
        "        # Add labels\n",
        "        for item in train_data:\n",
        "            for _, _, label in item[\"entities\"]:\n",
        "                ner.add_label(label)\n",
        "\n",
        "        # Training settings\n",
        "        optimizer = nlp.begin_training()\n",
        "\n",
        "        # Training loop\n",
        "        for itn in tqdm(range(n_iter), desc=\"Training\"):\n",
        "            random.shuffle(train_data)\n",
        "            losses = {}\n",
        "\n",
        "            # Batch training\n",
        "            batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
        "            for batch in batches:\n",
        "                examples = []\n",
        "                for text_data in batch:\n",
        "                    doc = nlp.make_doc(text_data[\"text\"])\n",
        "                    example = Example.from_dict(doc, {\"entities\": text_data[\"entities\"]})\n",
        "                    examples.append(example)\n",
        "\n",
        "                nlp.update(examples, drop=0.2, losses=losses)\n",
        "\n",
        "        # Save model\n",
        "        nlp.to_disk(output_dir)\n",
        "        return nlp\n",
        "\n",
        "def main():\n",
        "    trainer = NERTrainer()\n",
        "\n",
        "    # Load and clean data\n",
        "    data = trainer.load_and_clean_data(\"spacy_training_data.json\")\n",
        "\n",
        "    # Augment data\n",
        "    data = trainer.augment_data(data)\n",
        "\n",
        "    # Split data\n",
        "    random.shuffle(data)\n",
        "    split = int(len(data) * 0.8)\n",
        "    train_data = data[:split]\n",
        "    val_data = data[split:]\n",
        "\n",
        "    print(f\"\\nTraining examples: {len(train_data)}\")\n",
        "    print(f\"Validation examples: {len(val_data)}\")\n",
        "\n",
        "    # Create output directory\n",
        "    output_dir = \"trained_model\"\n",
        "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Train model\n",
        "    trainer.train_model(train_data, val_data, output_dir)\n",
        "    print(f\"\\nModel saved in: {output_dir}/\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euokkBoRHXhj",
        "outputId": "5baad626-11bc-4a10-e6c4-170809ba6321"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Initializing NER Trainer ===\n",
            "\n",
            "Loading and cleaning data from: spacy_training_data.json\n",
            "Loaded 17 examples, cleaned to 17 valid examples\n",
            "\n",
            "Augmenting training data...\n",
            "Original examples: 17\n",
            "Augmented examples: 32\n",
            "\n",
            "Training examples: 25\n",
            "Validation examples: 7\n",
            "\n",
            "Starting model training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"DETAILED STATEMENT\n",
            "Search\n",
            "Account Number\n",
            "634301509...\" with entities \"[(41, 53, 'ACC_NO'), (61, 80, 'PER')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "Training: 100%|██████████| 100/100 [01:07<00:00,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model saved in: trained_model/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy-lookups-data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSi8OYDmJsGN",
        "outputId": "9b82133b-de20-4046-a068-c9dd3e0ca7e4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spacy-lookups-data\n",
            "  Downloading spacy_lookups_data-1.0.5-py2.py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy-lookups-data) (75.1.0)\n",
            "Downloading spacy_lookups_data-1.0.5-py2.py3-none-any.whl (98.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.5/98.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: spacy-lookups-data\n",
            "Successfully installed spacy-lookups-data-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import fitz  # PyMuPDF\n",
        "from pathlib import Path\n",
        "import json\n",
        "from typing import Dict, List, Any\n",
        "import re\n",
        "\n",
        "class BankStatementExtractor:\n",
        "    def __init__(self, model_path: str):\n",
        "        print(f\"\\n=== Loading NER model from {model_path} ===\")\n",
        "        self.nlp = spacy.load(model_path)\n",
        "        print(\"✓ Model loaded successfully\")\n",
        "\n",
        "    def extract_text(self, pdf_path: str) -> str:\n",
        "        \"\"\"Extract text from PDF\"\"\"\n",
        "        print(f\"\\nExtracting text from: {pdf_path}\")\n",
        "\n",
        "        try:\n",
        "            with fitz.open(pdf_path) as doc:\n",
        "                text = \"\"\n",
        "                for page in doc:\n",
        "                    text += page.get_text()\n",
        "\n",
        "                # Clean text\n",
        "                text = re.sub(r'\\s+', ' ', text)\n",
        "                text = text.strip()\n",
        "                print(f\"✓ Extracted {len(text)} characters\")\n",
        "                return text\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error extracting text: {str(e)}\")\n",
        "            return \"\"\n",
        "\n",
        "    def process_statement(self, pdf_path: str) -> Dict[str, Any]:\n",
        "        \"\"\"Process a single bank statement\"\"\"\n",
        "        # Extract text\n",
        "        text = self.extract_text(pdf_path)\n",
        "        if not text:\n",
        "            return {\"error\": \"Failed to extract text\"}\n",
        "\n",
        "        print(\"\\nProcessing text with NER model...\")\n",
        "        doc = self.nlp(text)\n",
        "\n",
        "        # Extract entities\n",
        "        entities = []\n",
        "        for ent in doc.ents:\n",
        "            entities.append({\n",
        "                \"text\": ent.text,\n",
        "                \"label\": ent.label_,\n",
        "                \"start\": ent.start_char,\n",
        "                \"end\": ent.end_char\n",
        "            })\n",
        "\n",
        "        # Group by entity type\n",
        "        results = {\n",
        "            \"account_numbers\": [],\n",
        "            \"names\": [],\n",
        "            \"raw_entities\": entities\n",
        "        }\n",
        "\n",
        "        for ent in entities:\n",
        "            if ent[\"label\"] == \"ACC_NO\":\n",
        "                results[\"account_numbers\"].append(ent[\"text\"])\n",
        "            elif ent[\"label\"] == \"PER\":\n",
        "                results[\"names\"].append(ent[\"text\"])\n",
        "\n",
        "        print(\"\\nExtracted Entities:\")\n",
        "        print(f\"Account Numbers: {results['account_numbers']}\")\n",
        "        print(f\"Names: {results['names']}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "def test_single_pdf(model_path: str, pdf_path: str) -> None:\n",
        "    \"\"\"Test model on a single PDF\"\"\"\n",
        "    extractor = BankStatementExtractor(model_path)\n",
        "    results = extractor.process_statement(pdf_path)\n",
        "\n",
        "    # Save results\n",
        "    output_file = Path(pdf_path).stem + \"_extraction.json\"\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"\\nResults saved to: {output_file}\")\n",
        "\n",
        "def main():\n",
        "    # Paths\n",
        "    model_path = \"trained_model\"\n",
        "\n",
        "    # Test single PDF\n",
        "    pdf_path = \"/content/Axis 1-12-22 to 30-11-23.pdf\"\n",
        "    test_single_pdf(model_path, pdf_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XGDQOnzJ8y8",
        "outputId": "1a0f57ba-0522-4699-be8b-76cc4c96f337"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Loading NER model from trained_model ===\n",
            "✓ Model loaded successfully\n",
            "\n",
            "Extracting text from: /content/Axis 1-12-22 to 30-11-23.pdf\n",
            "✓ Extracted 133403 characters\n",
            "\n",
            "Processing text with NER model...\n",
            "\n",
            "Extracted Entities:\n",
            "Account Numbers: ['940810948', '922020023522300', '76813', '029979', '001398', '002625', '76823', '76827', '76831', '76826', '76842', '022463', '76849', '268304.54', '002706', '328132.79', '5900.00', '5900.00', '76863', '031148', '933510.57', '557030.09', '022626', '99067.29', '9440.00', '001604', '002781', '002798', '97896', '97897', '266508', '97898', '97900', '97899', '97901', '97902', '97908', '97907', '97909', '97912', '733017.77', '576716.84', '97914', '97913', '111332', '091642', '25724.00', '5900.00', '97917', '97916', '97911', '140959', '940094.81', '97918', '501998.33', '500529.33', '505409.33', '319007.33', '319757.33', '97920', '97921', '97923', '002905', '512995', '01930419229167', '035062969229001', '54402.72', '091747', '040488', '594194', '001694', '211570.84', '33329229713', '9198846296789060000', '339469', '213134139015000', '03009229022', '236830.40', '97927', '97932', '193631067689060000', '97928', '8843143949240000', '129687', '9440.00', '002970', '229042019485446', '001828', '001827', '97931', '97929', '97933', '97936', '97934', '001827', '9440.00', '9440.00', '003001', '97938', '04269579229536', '97940', '594366', '000814', '035062969229001 26238.48 CR', '97941', '8843143949240000', '97942', '000032', '8843143949240000', '193631067689060000', '76880', '30942', '97907', '1827', '220661', '1827 22066 31', '922020023522300', '1000']\n",
            "Names: ['171509.27 RTGS HUB', '182500.19 RTGS HUB', '227235.19 RTGS HUB', '209990.19 RTGS HUB', 'S/HDFC', 'S AMC TRADERS', 'S AMC TRADERS', 'S NAIM TOOLS', '553860.53 RTGS HUB', 'S NAIM TOOLS', 'S/UCO', 'S KHANDELWAL STEEL /UPI', 'S CUTTIN 509689.00', 'SUPPLIE/ 18880.00', 'S G And', 'S KHANDELWAL STEEL /UPI', 'S Amc Traders']\n",
            "\n",
            "Results saved to: Axis 1-12-22 to 30-11-23_extraction.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-M4o-9RPK0Ft",
        "outputId": "ddd98633-e2d8-4f95-f55e-4cd169c9ecb4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.24.13-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading PyMuPDF-1.24.13-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (19.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.24.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_best_matches(results: dict) -> dict:\n",
        "    \"\"\"Find the most likely primary account number and account holder name\"\"\"\n",
        "\n",
        "    def score_account_number(acc_no: str) -> float:\n",
        "        \"\"\"Score account numbers based on characteristics of typical bank account numbers\"\"\"\n",
        "        score = 0.0\n",
        "\n",
        "        # Skip if not a string\n",
        "        if not isinstance(acc_no, str):\n",
        "            return 0.0\n",
        "\n",
        "        # Length (most bank account numbers are between 10-16 digits)\n",
        "        clean_acc = acc_no.replace(\" \", \"\").replace(\"CR\", \"\").replace(\"DR\", \"\")\n",
        "        if 10 <= len(clean_acc) <= 16:\n",
        "            score += 3.0\n",
        "\n",
        "        # Contains only digits and spaces\n",
        "        if all(c.isdigit() or c.isspace() for c in clean_acc):\n",
        "            score += 2.0\n",
        "\n",
        "        # Not a transaction amount (doesn't contain decimal point)\n",
        "        if \".\" not in acc_no:\n",
        "            score += 2.0\n",
        "\n",
        "        # Not too short\n",
        "        if len(clean_acc) >= 8:\n",
        "            score += 1.0\n",
        "\n",
        "        # Appears early in the document\n",
        "        for entity in results.get(\"raw_entities\", []):\n",
        "            if entity[\"text\"] == acc_no and entity[\"label\"] == \"ACC_NO\":\n",
        "                if entity[\"start\"] < 1000:  # Early in document\n",
        "                    score += 2.0\n",
        "                break\n",
        "\n",
        "        return score\n",
        "\n",
        "    def score_name(name: str) -> float:\n",
        "        \"\"\"Score names based on characteristics of typical account holder names\"\"\"\n",
        "        score = 0.0\n",
        "\n",
        "        # Skip if not a string\n",
        "        if not isinstance(name, str):\n",
        "            return 0.0\n",
        "\n",
        "        # Not a transaction description\n",
        "        if not any(x in name.upper() for x in [\"RTGS\", \"NEFT\", \"UPI\", \"CR\", \"DR\", \"/\", \"SUPPLIE\", \"HUB\"]):\n",
        "            score += 3.0\n",
        "\n",
        "        # Contains multiple words (typical for full names)\n",
        "        if len(name.split()) >= 2:\n",
        "            score += 2.0\n",
        "\n",
        "        # Proper case (either Title case or UPPER case)\n",
        "        if name.istitle() or name.isupper():\n",
        "            score += 1.0\n",
        "\n",
        "        # Not too long (typical names are 2-4 words)\n",
        "        if len(name.split()) <= 4:\n",
        "            score += 1.0\n",
        "\n",
        "        # Appears early in document\n",
        "        for entity in results.get(\"raw_entities\", []):\n",
        "            if entity[\"text\"] == name and entity[\"label\"] == \"PER\":\n",
        "                if entity[\"start\"] < 1000:  # Early in document\n",
        "                    score += 2.0\n",
        "                break\n",
        "\n",
        "        return score\n",
        "\n",
        "    # Get lists from results\n",
        "    account_numbers = results.get(\"account_numbers\", [])\n",
        "    names = results.get(\"names\", [])\n",
        "\n",
        "    # Score and sort account numbers\n",
        "    acc_numbers = [(acc, score_account_number(acc))\n",
        "                  for acc in account_numbers]\n",
        "    acc_numbers = [(acc, score) for acc, score in acc_numbers if score > 0]\n",
        "    acc_numbers.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Score and sort names\n",
        "    names = [(name, score_name(name))\n",
        "            for name in names]\n",
        "    names = [(name, score) for name, score in names if score > 0]\n",
        "    names.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    best_matches = {\n",
        "        \"best_account_number\": {\n",
        "            \"text\": acc_numbers[0][0] if acc_numbers else None,\n",
        "            \"confidence\": acc_numbers[0][1] if acc_numbers else 0\n",
        "        },\n",
        "        \"best_name\": {\n",
        "            \"text\": names[0][0] if names else None,\n",
        "            \"confidence\": names[0][1] if names else 0\n",
        "        },\n",
        "        \"all_scored_accounts\": acc_numbers[:5],  # Top 5 matches\n",
        "        \"all_scored_names\": names[:5]  # Top 5 matches\n",
        "    }\n",
        "\n",
        "    return best_matches\n",
        "\n",
        "# Example usage:\n",
        "with open(\"/content/Axis 1-12-22 to 30-11-23_extraction.json\", \"r\") as f:\n",
        "    results = json.load(f)\n",
        "\n",
        "# Process the results directly (no need for indexing)\n",
        "best_matches = find_best_matches(results)  # Remove the [0]\n",
        "\n",
        "print(\"\\nBest Matches:\")\n",
        "print(f\"Account Number: {best_matches['best_account_number']['text']}\")\n",
        "print(f\"Confidence: {best_matches['best_account_number']['confidence']:.2f}\")\n",
        "print(f\"\\nAccount Holder: {best_matches['best_name']['text']}\")\n",
        "print(f\"Confidence: {best_matches['best_name']['confidence']:.2f}\")\n",
        "\n",
        "print(\"\\nTop 5 Account Numbers:\")\n",
        "for acc, score in best_matches['all_scored_accounts']:\n",
        "    print(f\"- {acc} (score: {score:.2f})\")\n",
        "\n",
        "print(\"\\nTop 5 Names:\")\n",
        "for name, score in best_matches['all_scored_names']:\n",
        "    print(f\"- {name} (score: {score:.2f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2npCiCZLK2UW",
        "outputId": "611539e8-62e2-43a7-ace6-b389e41647db"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best Matches:\n",
            "Account Number: 922020023522300\n",
            "Confidence: 10.00\n",
            "\n",
            "Account Holder: S AMC TRADERS\n",
            "Confidence: 7.00\n",
            "\n",
            "Top 5 Account Numbers:\n",
            "- 922020023522300 (score: 10.00)\n",
            "- 922020023522300 (score: 10.00)\n",
            "- 01930419229167 (score: 8.00)\n",
            "- 035062969229001 (score: 8.00)\n",
            "- 33329229713 (score: 8.00)\n",
            "\n",
            "Top 5 Names:\n",
            "- S AMC TRADERS (score: 7.00)\n",
            "- S AMC TRADERS (score: 7.00)\n",
            "- S NAIM TOOLS (score: 7.00)\n",
            "- S NAIM TOOLS (score: 7.00)\n",
            "- S CUTTIN 509689.00 (score: 7.00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mZUzJAdFMIRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import fitz  # PyMuPDF\n",
        "from pathlib import Path\n",
        "import json\n",
        "import re\n",
        "from typing import Dict, List, Any\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "class BankStatementProcessor:\n",
        "    def __init__(self, model_path: str = \"trained_model\"):\n",
        "        print(\"\\n=== Initializing Bank Statement Processor ===\")\n",
        "        self.nlp = spacy.load(model_path)\n",
        "\n",
        "    def process_directory(self, input_dir: str, output_dir: str = \"results\"):\n",
        "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Get all PDF files, excluding macOS hidden files\n",
        "        pdf_files = [\n",
        "            f for f in Path(input_dir).glob(\"**/*.pdf\")\n",
        "            if not f.name.startswith(\"._\")\n",
        "        ]\n",
        "\n",
        "        print(f\"\\nFound {len(pdf_files)} valid PDF files to process\")\n",
        "\n",
        "        results = []\n",
        "        for pdf_path in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
        "            try:\n",
        "                # Skip if file can't be opened (encrypted)\n",
        "                try:\n",
        "                    doc = fitz.open(pdf_path)\n",
        "                    doc.close()\n",
        "                except:\n",
        "                    print(f\"\\nSkipping encrypted/invalid PDF: {pdf_path.name}\")\n",
        "                    continue\n",
        "\n",
        "                result = self.process_single_pdf(pdf_path)\n",
        "                result[\"file_name\"] = pdf_path.name\n",
        "                results.append(result)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\nError processing {pdf_path.name}: {str(e)}\")\n",
        "                results.append({\n",
        "                    \"file_name\": pdf_path.name,\n",
        "                    \"error\": str(e)\n",
        "                })\n",
        "\n",
        "        self.save_results(results, output_dir)\n",
        "        return results\n",
        "\n",
        "    def process_single_pdf(self, pdf_path: str) -> Dict:\n",
        "        \"\"\"Process a single PDF file\"\"\"\n",
        "        # Extract text\n",
        "        text = self.extract_text(pdf_path)\n",
        "\n",
        "        # Extract entities\n",
        "        entities = self.extract_entities(text)\n",
        "\n",
        "        # Find best matches\n",
        "        best_matches = self.find_best_matches(entities)\n",
        "\n",
        "        return {\n",
        "            \"best_matches\": best_matches,\n",
        "            \"all_entities\": entities,\n",
        "            \"processed_at\": datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "    def extract_text(self, pdf_path: str) -> str:\n",
        "        \"\"\"Extract text from PDF\"\"\"\n",
        "        text = \"\"\n",
        "        with fitz.open(pdf_path) as doc:\n",
        "            for page in doc:\n",
        "                text += page.get_text()\n",
        "\n",
        "        return self.clean_text(text)\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        \"\"\"Clean extracted text\"\"\"\n",
        "        # Normalize whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "        # Remove special characters but keep essential ones\n",
        "        text = re.sub(r'[^\\w\\s\\-.,:/]', '', text)\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def extract_entities(self, text: str) -> Dict:\n",
        "        \"\"\"Extract entities using spaCy model\"\"\"\n",
        "        doc = self.nlp(text)\n",
        "\n",
        "        entities = {\n",
        "            \"account_numbers\": [],\n",
        "            \"names\": [],\n",
        "            \"raw_entities\": []\n",
        "        }\n",
        "\n",
        "        for ent in doc.ents:\n",
        "            entities[\"raw_entities\"].append({\n",
        "                \"text\": ent.text,\n",
        "                \"label\": ent.label_,\n",
        "                \"start\": ent.start_char,\n",
        "                \"end\": ent.end_char\n",
        "            })\n",
        "\n",
        "            if ent.label_ == \"ACC_NO\":\n",
        "                entities[\"account_numbers\"].append(ent.text)\n",
        "            elif ent.label_ == \"PER\":\n",
        "                entities[\"names\"].append(ent.text)\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def find_best_matches(self, entities: Dict) -> Dict:\n",
        "        \"\"\"Find best account number and name matches\"\"\"\n",
        "        def score_account_number(acc_no: str) -> float:\n",
        "            score = 0.0\n",
        "            clean_acc = acc_no.replace(\" \", \"\").replace(\"CR\", \"\").replace(\"DR\", \"\")\n",
        "\n",
        "            # Length check\n",
        "            if 10 <= len(clean_acc) <= 16:\n",
        "                score += 3.0\n",
        "\n",
        "            # Digit check\n",
        "            if all(c.isdigit() or c.isspace() for c in clean_acc):\n",
        "                score += 2.0\n",
        "\n",
        "            # Not a transaction amount\n",
        "            if \".\" not in acc_no:\n",
        "                score += 2.0\n",
        "\n",
        "            return score\n",
        "\n",
        "        def score_name(name: str) -> float:\n",
        "            score = 0.0\n",
        "\n",
        "            # Not a transaction description\n",
        "            if not any(x in name.upper() for x in [\"RTGS\", \"NEFT\", \"UPI\", \"CR\", \"DR\", \"/\", \"SUPPLIE\", \"HUB\"]):\n",
        "                score += 3.0\n",
        "\n",
        "            # Multiple words\n",
        "            if len(name.split()) >= 2:\n",
        "                score += 2.0\n",
        "\n",
        "            # Proper case\n",
        "            if name.istitle() or name.isupper():\n",
        "                score += 1.0\n",
        "\n",
        "            return score\n",
        "\n",
        "        # Score and sort\n",
        "        acc_numbers = [(acc, score_account_number(acc))\n",
        "                      for acc in entities[\"account_numbers\"]]\n",
        "        acc_numbers = [(acc, score) for acc, score in acc_numbers if score > 0]\n",
        "        acc_numbers.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        names = [(name, score_name(name))\n",
        "                for name in entities[\"names\"]]\n",
        "        names = [(name, score) for name, score in names if score > 0]\n",
        "        names.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        return {\n",
        "            \"account_number\": {\n",
        "                \"text\": acc_numbers[0][0] if acc_numbers else None,\n",
        "                \"confidence\": acc_numbers[0][1] if acc_numbers else 0\n",
        "            },\n",
        "            \"account_holder\": {\n",
        "                \"text\": names[0][0] if names else None,\n",
        "                \"confidence\": names[0][1] if names else 0\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def save_results(self, results: List[Dict], output_dir: str):\n",
        "        \"\"\"Save results in multiple formats\"\"\"\n",
        "        # Save detailed JSON\n",
        "        json_path = Path(output_dir) / \"detailed_results.json\"\n",
        "        with open(json_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        # Create simplified CSV\n",
        "        csv_data = []\n",
        "        for result in results:\n",
        "            if \"error\" in result:\n",
        "                csv_data.append({\n",
        "                    \"file_name\": result[\"file_name\"],\n",
        "                    \"account_number\": \"ERROR\",\n",
        "                    \"account_holder\": \"ERROR\",\n",
        "                    \"error\": result[\"error\"]\n",
        "                })\n",
        "            else:\n",
        "                csv_data.append({\n",
        "                    \"file_name\": result[\"file_name\"],\n",
        "                    \"account_number\": result[\"best_matches\"][\"account_number\"][\"text\"],\n",
        "                    \"account_holder\": result[\"best_matches\"][\"account_holder\"][\"text\"],\n",
        "                    \"account_number_confidence\": result[\"best_matches\"][\"account_number\"][\"confidence\"],\n",
        "                    \"account_holder_confidence\": result[\"best_matches\"][\"account_holder\"][\"confidence\"]\n",
        "                })\n",
        "\n",
        "        # Save CSV\n",
        "        df = pd.DataFrame(csv_data)\n",
        "        csv_path = Path(output_dir) / \"extracted_data.csv\"\n",
        "        df.to_csv(csv_path, index=False)\n",
        "\n",
        "        print(f\"\\nResults saved to:\")\n",
        "        print(f\"- Detailed JSON: {json_path}\")\n",
        "        print(f\"- CSV Summary: {csv_path}\")\n",
        "\n",
        "def test_single_pdf(model_path: str, pdf_path: str):\n",
        "    \"\"\"Test the model on a single PDF\"\"\"\n",
        "    print(f\"\\n=== Testing Single PDF: {Path(pdf_path).name} ===\")\n",
        "\n",
        "    processor = BankStatementProcessor(model_path)\n",
        "\n",
        "    try:\n",
        "        # Extract text first\n",
        "        text = processor.extract_text(pdf_path)\n",
        "        print(\"\\nExtracted Text (first 500 characters):\")\n",
        "        print(\"=\"*80)\n",
        "        print(text[:500])\n",
        "        print(\"=\"*80)\n",
        "        print(f\"\\nTotal text length: {len(text)} characters\")\n",
        "\n",
        "        # Process the text\n",
        "        result = processor.process_single_pdf(pdf_path)\n",
        "\n",
        "        print(\"\\nExtracted Information:\")\n",
        "        print(f\"Account Number: {result['best_matches']['account_number']['text']}\")\n",
        "        print(f\"Confidence: {result['best_matches']['account_number']['confidence']:.2f}\")\n",
        "        print(f\"\\nAccount Holder: {result['best_matches']['account_holder']['text']}\")\n",
        "        print(f\"Confidence: {result['best_matches']['account_holder']['confidence']:.2f}\")\n",
        "\n",
        "        print(\"\\nAll Found Account Numbers with Context:\")\n",
        "        for entity in result['all_entities']['raw_entities']:\n",
        "            if entity['label'] == 'ACC_NO':\n",
        "                start = max(0, entity['start'] - 30)\n",
        "                end = min(len(text), entity['end'] + 30)\n",
        "                context = text[start:end]\n",
        "                print(f\"\\n- {entity['text']}\")\n",
        "                print(f\"  Context: ...{context}...\")\n",
        "\n",
        "        print(\"\\nAll Found Names with Context:\")\n",
        "        for entity in result['all_entities']['raw_entities']:\n",
        "            if entity['label'] == 'PER':\n",
        "                start = max(0, entity['start'] - 30)\n",
        "                end = min(len(text), entity['end'] + 30)\n",
        "                context = text[start:end]\n",
        "                print(f\"\\n- {entity['text']}\")\n",
        "                print(f\"  Context: ...{context}...\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing PDF: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def main():\n",
        "    # Configuration\n",
        "    input_dir = \"/content/SHORT_PAGES_BANK_STATEMENTS -2\"\n",
        "    output_dir = \"results\"\n",
        "    model_path = \"trained_model\"\n",
        "\n",
        "    # Ask user what to do\n",
        "    print(\"\\n1. Process entire directory\")\n",
        "    print(\"2. Test single PDF\")\n",
        "    choice = input(\"\\nEnter your choice (1 or 2): \")\n",
        "\n",
        "    if choice == \"1\":\n",
        "        # Process directory\n",
        "        processor = BankStatementProcessor(model_path)\n",
        "        results = processor.process_directory(input_dir, output_dir)\n",
        "\n",
        "        # Print summary\n",
        "        print(\"\\nProcessing Summary:\")\n",
        "        total = len(results)\n",
        "        errors = sum(1 for r in results if \"error\" in r)\n",
        "        successful = total - errors\n",
        "\n",
        "        print(f\"Total files processed: {total}\")\n",
        "        print(f\"Successful: {successful} ({successful/total*100:.1f}%)\")\n",
        "        print(f\"Errors: {errors} ({errors/total*100:.1f}%)\")\n",
        "\n",
        "    elif choice == \"2\":\n",
        "        # Test single PDF\n",
        "        pdf_path = \"/content/aiyaz jupiter.pdf\"\n",
        "        test_single_pdf(model_path, pdf_path)\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid choice!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Km4zVKZVNPA5",
        "outputId": "c0ac86aa-a985-45ef-fb3f-e18213d89640"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1. Process entire directory\n",
            "2. Test single PDF\n",
            "\n",
            "Enter your choice (1 or 2): 2\n",
            "\n",
            "=== Testing Single PDF: aiyaz jupiter.pdf ===\n",
            "\n",
            "=== Initializing Bank Statement Processor ===\n",
            "\n",
            "Extracted Text (first 500 characters):\n",
            "================================================================================\n",
            "247 Phone Banking 1800 425 1199 1800 420 1199 www.federal.co.in Name AIYAZ ANWAR QURESHI Branch Name Fintech Partnerships Jupiter Communication Address plot no 1/5/35, gajanan colony, govandi, abdul hamid marg, Mumbai, Maharashtra, 400043 Branch Address Federal Bank Ltd, Fintech Partnerships Department Integrated Startup Complex, KSUM, HMT Colony, Kalamassery, Ernakulam, Kerala - 683503 Address Last Updated On 17/01/2022 Branch Sol ID 7777 Registered Mobile Number 917738810525 Account Number 777\n",
            "================================================================================\n",
            "\n",
            "Total text length: 33149 characters\n",
            "\n",
            "Extracted Information:\n",
            "Account Number: 917738810525\n",
            "Confidence: 7.00\n",
            "\n",
            "Account Holder: None\n",
            "Confidence: 0.00\n",
            "\n",
            "All Found Account Numbers with Context:\n",
            "\n",
            "- 917738810525\n",
            "  Context: ...7777 Registered Mobile Number 917738810525 Account Number 77770106311893...\n",
            "\n",
            "- 0.35\n",
            "  Context: ...iyaz123-8okici/0000 TFR 27.00 0.35 Dr 08/11/2023 08/11/2023 UPI ...\n",
            "\n",
            "- 0.35\n",
            "  Context: ...az123-8okici/0000 TFR 1400.00 0.35 Dr Page 1 of 14 The Federal B...\n",
            "\n",
            "- 0484 2630996 Website\n",
            "  Context: ..., Kerala 683101 Phone number: 0484 2630996 Website: www.federal.co.in 247 Phone ...\n",
            "\n",
            "- 0.35\n",
            "  Context: ...AL ALEU L UP MUHA TFR 2000.00 0.35 Dr 26/12/2023 27/12/2023 UPI ...\n",
            "\n",
            "- 0.35\n",
            "  Context: ...az123-8okici/0000 TFR 1400.00 0.35 Dr 27/12/2023 27/12/2023 UPI ...\n",
            "\n",
            "- 0.35\n",
            "  Context: ... BANK LTD. SHIVAJ TFR 1400.00 0.35 Dr 20/01/2024 21/01/2024 UPI ...\n",
            "\n",
            "- 0.35\n",
            "  Context: ...yaz123-7okhdf/0000 TFR 271.00 0.35 Dr 27/01/2024 28/01/2024 UPI ...\n",
            "\n",
            "- 0.35\n",
            "  Context: ...yaz123-7okhdf/0000 TFR 224.00 0.35 Dr 29/01/2024 29/01/2024 UPI ...\n",
            "\n",
            "- 0484 2630996 Website\n",
            "  Context: ..., Kerala 683101 Phone number: 0484 2630996 Website: www.federal.co.in 247 Phone ...\n",
            "\n",
            "- 24.35\n",
            "  Context: ...reshi2022-1okh/0000 TFR 20.00 24.35 Dr 02/02/2024 02/02/2024 UPI ...\n",
            "\n",
            "- 24.35\n",
            "  Context: ...S BANK LIMIT LOTUS TFR 500.00 24.35 Dr 03/02/2024 03/02/2024 UPI ...\n",
            "\n",
            "- 24.35\n",
            "  Context: ... BANK LTD. SHIVAJ TFR 2000.00 24.35 Dr 03/02/2024 04/02/2024 UPI ...\n",
            "\n",
            "- 24.35\n",
            "  Context: ...5glw953bpaytm//5814 TFR 80.00 24.35 Dr 04/02/2024 04/02/2024 UPI ...\n",
            "\n",
            "- 0484 2630996 Website\n",
            "  Context: ..., Kerala 683101 Phone number: 0484 2630996 Website: www.federal.co.in 247 Phone ...\n",
            "\n",
            "- 3225.35\n",
            "  Context: ...g6rknndkpaytm//5812 TFR 10.00 3225.35 Dr 06/02/2024 06/02/2024 UPIO...\n",
            "\n",
            "- 2431.45\n",
            "  Context: ...PLUG TECHNOLOGIES PR TFR 1.00 2431.45 Cr 06/02/2024 07/02/2024 UPIO...\n",
            "\n",
            "- 0484 2630996 Website\n",
            "  Context: ..., Kerala 683101 Phone number: 0484 2630996 Website: www.federal.co.in 247 Phone ...\n",
            "\n",
            "- 0484 2630996 Website\n",
            "  Context: ..., Kerala 683101 Phone number: 0484 2630996 Website: www.federal.co.in 247 Phone ...\n",
            "\n",
            "- 0484 2630996 Website\n",
            "  Context: ..., Kerala 683101 Phone number: 0484 2630996 Website: www.federal.co.in 247 Phone ...\n",
            "\n",
            "- 0.45\n",
            "  Context: ...az123-7okhdf/0000 TFR 6720.00 0.45 Dr 15/02/2024 15/02/2024 UPI ...\n",
            "\n",
            "- 0.45\n",
            "  Context: ...iyaz123-7okhdf/0000 TFR 80.00 0.45 Dr 23/02/2024 23/02/2024 UPI ...\n",
            "\n",
            "- 0.45\n",
            "  Context: ... BANK LIMIT LOTUS TFR 5000.00 0.45 Dr 23/02/2024 23/02/2024 UPI ...\n",
            "\n",
            "- 0484 2630996 Website\n",
            "  Context: ..., Kerala 683101 Phone number: 0484 2630996 Website: www.federal.co.in 247 Phone ...\n",
            "\n",
            "- 2378.45\n",
            "  Context: ...az123-3oksbi/0000 TFR 1938.00 2378.45 Cr 27/02/2024 27/02/2024 UPIO...\n",
            "\n",
            "- 0484 2630996 Website\n",
            "  Context: ..., Kerala 683101 Phone number: 0484 2630996 Website: www.federal.co.in 247 Phone ...\n",
            "\n",
            "- 0484 2630996 Website\n",
            "  Context: ..., Kerala 683101 Phone number: 0484 2630996 Website: www.federal.co.in 247 Phone ...\n",
            "\n",
            "- 0484 2630996 Website\n",
            "  Context: ..., Kerala 683101 Phone number: 0484 2630996 Website: www.federal.co.in 247 Phone ...\n",
            "\n",
            "- 2117.45\n",
            "  Context: ...eqicici/CHEQ/0000 TFR 2000.00 2117.45 Cr 06/03/2024 06/03/2024 TO A...\n",
            "\n",
            "- 0484 2630996 Website\n",
            "  Context: ..., Kerala 683101 Phone number: 0484 2630996 Website: www.federal.co.in 247 Phone ...\n",
            "\n",
            "- 0484 2630996 Website\n",
            "  Context: ..., Kerala 683101 Phone number: 0484 2630996 Website: www.federal.co.in 247 Phone ...\n",
            "\n",
            "- 0.18\n",
            "  Context: ...iyaz123-7okhdf/0000 TFR 75.00 0.18 Dr 21/03/2024 21/03/2024 UPI ...\n",
            "\n",
            "- 0.18\n",
            "  Context: ...BANK LIMIT SAMRAJ TFR 4000.00 0.18 Dr 22/03/2024 22/03/2024 UPI ...\n",
            "\n",
            "- 0.18\n",
            "  Context: ...S BANK LIMIT LOTUS TFR 500.00 0.18 Dr 23/03/2024 22/03/2024 SBIN...\n",
            "\n",
            "- 202477770106311893\n",
            "  Context: ...024 SBINT:23-12-2023 to 22-03-202477770106311893 TFR 7.00 7.18 Cr Page 13 of 1...\n",
            "\n",
            "- 0484 2630996 Website\n",
            "  Context: ..., Kerala 683101 Phone number: 0484 2630996 Website: www.federal.co.in 247 Phone ...\n",
            "\n",
            "- 0484 2630996 Website\n",
            "  Context: ..., Kerala 683101 Phone number: 0484 2630996 Website: www.federal.co.in...\n",
            "\n",
            "All Found Names with Context:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Path to the ZIP file\n",
        "zip_path = '/content/SHORT_PAGES_BANK_STATEMENTS -2.zip'\n",
        "\n",
        "# Destination directory for extracted files\n",
        "extract_to = '/content/SHORT_PAGES_BANK_STATEMENTS -2'\n",
        "\n",
        "# Extract the ZIP file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)\n",
        "\n",
        "print(f\"Files extracted to: {extract_to}\")\n",
        "\n",
        "# List all files to verify\n",
        "for root, dirs, files in os.walk(extract_to):\n",
        "    for file in files:\n",
        "        print(os.path.join(root, file))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVvNjoeqNhRo",
        "outputId": "ce8d9680-8ca3-457c-aa16-e54d332d0384"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files extracted to: /content/SHORT_PAGES_BANK_STATEMENTS -2\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/SHORT_PAGES_BANK_STATEMENTS -2/KOTAK_BANK/CPSPM_40108290_1704838422.PDF\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/SHORT_PAGES_BANK_STATEMENTS -2/Canara_BANK/BCCB-1 (1).pdf\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/SHORT_PAGES_BANK_STATEMENTS -2/Canara_BANK/Canara 2 format .pdf\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/SHORT_PAGES_BANK_STATEMENTS -2/TJSB_BANK/Copy of Copy of TJSB.pdf\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/SHORT_PAGES_BANK_STATEMENTS -2/BHARAT_BALAJI_BANK/AccountStatement_Evolve - 2023-12-27T134118.679.pdf\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/SHORT_PAGES_BANK_STATEMENTS -2/BHARAT_BALAJI_BANK/Bharat_Balaji_1Jan24_23Feb24.pdf\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/SHORT_PAGES_BANK_STATEMENTS -2/PNB_BANK/Copy of pnbbank.pdf\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/SHORT_PAGES_BANK_STATEMENTS -2/YES_BANK/yesbank.pdf\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/SHORT_PAGES_BANK_STATEMENTS -2/YES_BANK/Yes bank (1).pdf\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/SHORT_PAGES_BANK_STATEMENTS -2/SBI_BANK/Mar-24.pdf\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/SHORT_PAGES_BANK_STATEMENTS -2/ICICI_BANK/Statement_DEC2023_777705907080.pdf\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/SHORT_PAGES_BANK_STATEMENTS -2/ICICI_BANK/Copy of ICICI 9727 acc statement.pdf\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/SHORT_PAGES_BANK_STATEMENTS -2/AXIS_BANK/AXIS_STATEMENT_CLOSURE_X940.pdf\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/SHORT_PAGES_BANK_STATEMENTS -2/AXIS_BANK/Axis 3rd Jan 24 to 9th March 24 Account No ( 2767 ).pdf\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/SHORT_PAGES_BANK_STATEMENTS -2/IDFC_BANK/IDFCFIRSTBankstatement_10083065412_155547128.pdf\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/SHORT_PAGES_BANK_STATEMENTS -2/NKGSB_BANK/Copy of Copy of NKGSB.pdf\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/SHORT_PAGES_BANK_STATEMENTS -2/IDBI_BANK/IDBI APRL.pdf\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/SHORT_PAGES_BANK_STATEMENTS -2/SC_BANK/Apr-22 SC Bank Statement.pdf\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/SHORT_PAGES_BANK_STATEMENTS -2/HDFC_BANK/27JAN24_23FEB24_HDFC_BALAJI.pdf\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/SHORT_PAGES_BANK_STATEMENTS -2/Union_BANK/Copy of Copy of Bank Statement Union Bank Dec.pdf\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/SHORT_PAGES_BANK_STATEMENTS -2/BOI_BANK/1602202413233700_97675273.pdf\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/SHORT_PAGES_BANK_STATEMENTS -2/BOM_BANK/BOM_Statement (1).PDF\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/SHORT_PAGES_BANK_STATEMENTS -2/BOM_BANK/bom.pdf\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/SHORT_PAGES_BANK_STATEMENTS -2/BASSEIN_CATHOLIC_BANK/BASSEIN CATHOLIC.pdf\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/__MACOSX/._SHORT_PAGES_BANK_STATEMENTS -2\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/__MACOSX/SHORT_PAGES_BANK_STATEMENTS -2/._SC_BANK\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/__MACOSX/SHORT_PAGES_BANK_STATEMENTS -2/._BASSEIN_CATHOLIC_BANK\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/__MACOSX/SHORT_PAGES_BANK_STATEMENTS -2/._Canara_BANK\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/__MACOSX/SHORT_PAGES_BANK_STATEMENTS -2/._HDFC_BANK\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/__MACOSX/SHORT_PAGES_BANK_STATEMENTS -2/._IDFC_BANK\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/__MACOSX/SHORT_PAGES_BANK_STATEMENTS -2/._PNB_BANK\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/__MACOSX/SHORT_PAGES_BANK_STATEMENTS -2/._TJSB_BANK\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/__MACOSX/SHORT_PAGES_BANK_STATEMENTS -2/._KOTAK_BANK\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/__MACOSX/SHORT_PAGES_BANK_STATEMENTS -2/._YES_BANK\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/__MACOSX/SHORT_PAGES_BANK_STATEMENTS -2/._NKGSB_BANK\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/__MACOSX/SHORT_PAGES_BANK_STATEMENTS -2/._BHARAT_BALAJI_BANK\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/__MACOSX/SHORT_PAGES_BANK_STATEMENTS -2/._Union_BANK\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/__MACOSX/SHORT_PAGES_BANK_STATEMENTS -2/._SBI_BANK\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/__MACOSX/SHORT_PAGES_BANK_STATEMENTS -2/._ICICI_BANK\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/__MACOSX/SHORT_PAGES_BANK_STATEMENTS -2/._IDBI_BANK\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/__MACOSX/SHORT_PAGES_BANK_STATEMENTS -2/._AXIS_BANK\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/__MACOSX/SHORT_PAGES_BANK_STATEMENTS -2/._BOM_BANK\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/__MACOSX/SHORT_PAGES_BANK_STATEMENTS -2/._BOI_BANK\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/__MACOSX/SHORT_PAGES_BANK_STATEMENTS -2/KOTAK_BANK/._CPSPM_40108290_1704838422.PDF\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/__MACOSX/SHORT_PAGES_BANK_STATEMENTS -2/Canara_BANK/._BCCB-1 (1).pdf\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/__MACOSX/SHORT_PAGES_BANK_STATEMENTS -2/Canara_BANK/._Canara 2 format .pdf\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/__MACOSX/SHORT_PAGES_BANK_STATEMENTS -2/TJSB_BANK/._Copy of Copy of TJSB.pdf\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/__MACOSX/SHORT_PAGES_BANK_STATEMENTS -2/BHARAT_BALAJI_BANK/._AccountStatement_Evolve - 2023-12-27T134118.679.pdf\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/__MACOSX/SHORT_PAGES_BANK_STATEMENTS -2/BHARAT_BALAJI_BANK/._Bharat_Balaji_1Jan24_23Feb24.pdf\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/__MACOSX/SHORT_PAGES_BANK_STATEMENTS -2/PNB_BANK/._Copy of pnbbank.pdf\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/__MACOSX/SHORT_PAGES_BANK_STATEMENTS -2/YES_BANK/._Yes bank (1).pdf\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/__MACOSX/SHORT_PAGES_BANK_STATEMENTS -2/YES_BANK/._yesbank.pdf\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/__MACOSX/SHORT_PAGES_BANK_STATEMENTS -2/SBI_BANK/._Mar-24.pdf\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/__MACOSX/SHORT_PAGES_BANK_STATEMENTS -2/ICICI_BANK/._Statement_DEC2023_777705907080.pdf\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/__MACOSX/SHORT_PAGES_BANK_STATEMENTS -2/ICICI_BANK/._Copy of ICICI 9727 acc statement.pdf\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/__MACOSX/SHORT_PAGES_BANK_STATEMENTS -2/AXIS_BANK/._AXIS_STATEMENT_CLOSURE_X940.pdf\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/__MACOSX/SHORT_PAGES_BANK_STATEMENTS -2/AXIS_BANK/._Axis 3rd Jan 24 to 9th March 24 Account No ( 2767 ).pdf\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/__MACOSX/SHORT_PAGES_BANK_STATEMENTS -2/IDFC_BANK/._IDFCFIRSTBankstatement_10083065412_155547128.pdf\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/__MACOSX/SHORT_PAGES_BANK_STATEMENTS -2/NKGSB_BANK/._Copy of Copy of NKGSB.pdf\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/__MACOSX/SHORT_PAGES_BANK_STATEMENTS -2/IDBI_BANK/._IDBI APRL.pdf\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/__MACOSX/SHORT_PAGES_BANK_STATEMENTS -2/SC_BANK/._Apr-22 SC Bank Statement.pdf\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/__MACOSX/SHORT_PAGES_BANK_STATEMENTS -2/HDFC_BANK/._27JAN24_23FEB24_HDFC_BALAJI.pdf\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/__MACOSX/SHORT_PAGES_BANK_STATEMENTS -2/Union_BANK/._Copy of Copy of Bank Statement Union Bank Dec.pdf\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/__MACOSX/SHORT_PAGES_BANK_STATEMENTS -2/BOI_BANK/._1602202413233700_97675273.pdf\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/__MACOSX/SHORT_PAGES_BANK_STATEMENTS -2/BOM_BANK/._bom.pdf\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/__MACOSX/SHORT_PAGES_BANK_STATEMENTS -2/BOM_BANK/._BOM_Statement (1).PDF\n",
            "/content/SHORT_PAGES_BANK_STATEMENTS -2/__MACOSX/SHORT_PAGES_BANK_STATEMENTS -2/BASSEIN_CATHOLIC_BANK/._BASSEIN CATHOLIC.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eH4GxV76NhoF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}